{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import segmentation_models_pytorch as smp\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.functional import F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.transforms import transforms\n",
    "from safetensors.torch import save_file, load_file\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from decord import VideoReader, cpu\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base paths\n",
    "ROOT_DIR = os.path.join(os.getcwd(), os.pardir)\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data')\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'unet_segmentation_train', 'road_marking')\n",
    "TRAIN_VIDEOS_DIR = os.path.join(DATA_DIR, 'train_videos')\n",
    "TRAIN_TARGETS_PATH = os.path.join(DATA_DIR, 'train_targets.csv')\n",
    "UNET_CHECKPOINT_PATH = os.path.join(ROOT_DIR, 'checkpoints', 'unet_segment_resnet50.safetensors')\n",
    "YOLO_SIGNS_CHECKPOINT_PATH = os.path.join(ROOT_DIR, 'checkpoints', 'road-signs-yolov8n.pt')\n",
    "YOLO_TRAFFIC_LIGHTS_CHECKPOINT_PATH = os.path.join(ROOT_DIR, 'checkpoints', 'traffic-lights-yolov8n.pt')\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "LEARN_FROM_ZERO = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, vid_dir, targets, width, height, mask_encoder, \n",
    "                 targets_encoder=None, disc_freq=5, device='cpu'):\n",
    "        self.vid_dir = vid_dir\n",
    "        self.videos_names = []\n",
    "        self.videos_len = []\n",
    "        self.fps = []\n",
    "        self.videos_samples_len = []\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.device=device\n",
    "        self.mask_encoder = mask_encoder\n",
    "        \n",
    "        # Preload video metadata and prepare decoders\n",
    "        for filename in os.listdir(vid_dir):\n",
    "            if not filename.endswith('.txt'):\n",
    "                filepath = os.path.join(vid_dir, filename)\n",
    "                self.videos_names.append(filename)\n",
    "\n",
    "                with open(filepath, 'rb') as f:\n",
    "                    video_reader = VideoReader(f, ctx=cpu(0))\n",
    "\n",
    "                self.videos_len.append(len(video_reader))\n",
    "                self.fps.append(video_reader.get_avg_fps())\n",
    "\n",
    "                assert disc_freq <= self.fps[-1]\n",
    "                self.videos_samples_len.append(int(self.videos_len[-1] / self.fps[-1] * disc_freq))\n",
    "        \n",
    "        self.disc_freq = disc_freq\n",
    "        self.targets = targets\n",
    "\n",
    "        # Encode targets if encoder provided\n",
    "        if targets_encoder:\n",
    "            self.targets_encoder = targets_encoder\n",
    "            self.targets = self.targets_encoder.transform(self.targets)\n",
    "        else:\n",
    "            self.targets_encoder = LabelEncoder()\n",
    "            self.targets['violation'] = self.targets_encoder.fit_transform(self.targets['violation'])\n",
    "        \n",
    "        self.video = [None, None]\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(self.videos_samples_len)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Determine which video this idx falls into\n",
    "        for vid_idx in range(len(self.videos_names)):\n",
    "            if idx >= self.videos_samples_len[vid_idx]:\n",
    "                idx -= self.videos_samples_len[vid_idx]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if vid_idx != self.video[0]:\n",
    "            if self.video[1]:\n",
    "                self.video[1].seek(0)\n",
    "            with open(os.path.join(self.vid_dir, self.videos_names[vid_idx]), 'rb') as f:\n",
    "                self.video = [vid_idx, VideoReader(f, ctx=cpu(0))]\n",
    "        \n",
    "        # Calculate the frame index based on disc_freq\n",
    "        frame_sec = int(idx / self.disc_freq - 1e-8)\n",
    "        frame_idx = int(idx / self.disc_freq * self.fps[vid_idx])\n",
    "        \n",
    "        # Use Decord to fetch the frame efficiently\n",
    "        video_reader = self.video[1]\n",
    "        frame = video_reader[frame_idx].asnumpy()\n",
    "        \n",
    "        # Preprocess image\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_CUBIC)   \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = Image.fromarray(frame)\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        frame = transform(frame)\n",
    "        frame = frame.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mask_transform = transforms.Resize((164, 164), interpolation=transforms.InterpolationMode.BICUBIC)\n",
    "            mask = self.mask_encoder(frame.unsqueeze(0))\n",
    "            mask = mask.squeeze(1)\n",
    "            mask = mask_transform(mask)\n",
    "            mask = mask.to(torch.uint8)\n",
    "            mask = mask.squeeze(0)\n",
    "            mask = mask.to(torch.float32)\n",
    "\n",
    "        # Extract the label for the current frame\n",
    "        label = self.targets['violation'][(self.targets['id'] == self.videos_names[vid_idx].split('.')[0].lower()) \n",
    "                                          & (self.targets['time'] == frame_sec + 1)]\n",
    "        assert len(label) == 1\n",
    "        label = torch.tensor(label.iloc[0])\n",
    "        \n",
    "        return frame, mask, label.to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load unet roadline segmentation model\n",
    "unet_state = load_file(UNET_CHECKPOINT_PATH)\n",
    "unet_model = smp.Unet(\n",
    "    encoder_name='resnet50',\n",
    "    in_channels=3,\n",
    "    classes=1\n",
    ").to(DEVICE)\n",
    "unet_model.load_state_dict(unet_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yolo8 detection models\n",
    "yolo_signs = YOLO(YOLO_SIGNS_CHECKPOINT_PATH)\n",
    "yolo_traffic_lights = YOLO(YOLO_TRAFFIC_LIGHTS_CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load backbones for frames and mask\n",
    "frame_backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViolationDetectionModel(nn.Module):\n",
    "    def __init__(self, frame_backbone):\n",
    "        super().__init__()\n",
    "        self.frame_backbone = frame_backbone\n",
    "        self.seq_frame_in = self._make_sequential(1000, 512)\n",
    "        self.seq_mask_in = self._make_sequential(164 * 164, 2048)\n",
    "        self.seq_combine = self._make_sequential(2048 + 512, 1024)\n",
    "        self.fc1 = self._make_sequential(1024, 512)\n",
    "        self.fc2 = self._make_sequential(512, 256)\n",
    "        self.fc3 = self._make_sequential(256, 128)\n",
    "        self.fc_out = self._make_sequential(128, 6)\n",
    "        \n",
    "    def forward(self, frame, mask):\n",
    "        # flatten mask\n",
    "        mask = torch.flatten(mask, start_dim=1)\n",
    "\n",
    "        # pass frame and its mask through backbones\n",
    "        frame_backbone_out = self.frame_backbone(frame)\n",
    "        mask_out = self.seq_mask_in(mask)\n",
    "\n",
    "        # combine outputs\n",
    "        frame_out = self.seq_frame_in(frame_backbone_out)\n",
    "        combined = self.seq_combine(torch.hstack([frame_out, mask_out]))\n",
    "\n",
    "        # get class probs\n",
    "        out = self.fc1(combined)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.fc_out(out)\n",
    "        out = F.softmax(out, dim=-1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _make_sequential(self, input_dim, output_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.ReLU()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.read_csv(TRAIN_TARGETS_PATH)\n",
    "dataset = TrainDataset(\n",
    "    vid_dir=TRAIN_VIDEOS_DIR,\n",
    "    targets=targets, \n",
    "    disc_freq=5,\n",
    "    width=736,\n",
    "    height=416,\n",
    "    mask_encoder=unet_model,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 1045/1875 [04:50<03:54,  3.55it/s, loss=-0.896, total_loss=-0.679, acc=0.848]"
     ]
    }
   ],
   "source": [
    "NUM_EPOCH = 10\n",
    "\n",
    "model = ViolationDetectionModel(\n",
    "    frame_backbone=frame_backbone\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    pbar = tqdm(dataloader)\n",
    "    total_loss = 0\n",
    "    accuracy = 0\n",
    "    for i, batch in enumerate(pbar, start=1):\n",
    "        frame, mask, target = batch\n",
    "        out = model(frame, mask)\n",
    "        loss = criterion(out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss\n",
    "        accuracy += (torch.argmax(out, dim=-1) == target).sum()\n",
    "        pbar.set_postfix({'loss': loss.item(), 'total_loss': total_loss.item() / i, 'acc': (accuracy.item() / 8) / i})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
