{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.functional import F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from safetensors.torch import save_file, load_file\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from decord import VideoReader, cpu\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, vid_dir, targets, targets_encoder=None, disc_freq=5):\n",
    "        self.videos_names = []\n",
    "        self.videos = []\n",
    "        self.videos_len = []\n",
    "        self.fps = []\n",
    "        self.videos_samples_len = []\n",
    "        \n",
    "        # Preload video metadata and prepare decoders\n",
    "        for filename in os.listdir(vid_dir):\n",
    "            if not filename.endswith('.txt'):\n",
    "                filepath = os.path.join(vid_dir, filename)\n",
    "                self.videos_names.append(filename)\n",
    "                video_reader = VideoReader(filepath, ctx=cpu(0))  # Decord VideoReader for fast access\n",
    "                self.videos.append(video_reader)\n",
    "                self.videos_len.append(len(video_reader))\n",
    "                self.fps.append(video_reader.get_avg_fps())\n",
    "                assert disc_freq <= self.fps[-1]\n",
    "                self.videos_samples_len.append(int(self.videos_len[-1] / self.fps[-1] * disc_freq))\n",
    "        \n",
    "        self.disc_freq = disc_freq\n",
    "        self.targets = targets\n",
    "\n",
    "        # Encode targets if encoder provided\n",
    "        if targets_encoder:\n",
    "            self.targets_encoder = targets_encoder\n",
    "            self.targets = self.targets_encoder.transform(self.targets)\n",
    "        else:\n",
    "            self.targets_encoder = LabelEncoder()\n",
    "            self.targets['violation'] = self.targets_encoder.fit_transform(self.targets['violation'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(self.videos_samples_len)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Determine which video this idx falls into\n",
    "        for vid_idx in range(len(self.videos)):\n",
    "            if idx >= self.videos_samples_len[vid_idx]:\n",
    "                idx -= self.videos_samples_len[vid_idx]\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # Calculate the frame index based on disc_freq\n",
    "        frame_sec = int(idx / self.disc_freq - 1e-8)\n",
    "        frame_idx = int(idx / self.disc_freq * self.fps[vid_idx])\n",
    "        \n",
    "        # Use Decord to fetch the frame efficiently\n",
    "        video_reader = self.videos[vid_idx]\n",
    "        frame = video_reader[frame_idx].asnumpy()\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        frame = cv2.resize(frame, (736, 416), interpolation=cv2.INTER_CUBIC)   \n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = Image.fromarray(frame)\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        frame = transform(frame)\n",
    "\n",
    "        # Extract the label for the current frame\n",
    "        label = self.targets['violation'][(self.targets['id'] == self.videos_names[vid_idx].split('.')[0].lower()) & (self.targets['time'] == frame_sec + 1)]\n",
    "        assert len(label) == 1\n",
    "        label = label.iloc[0]\n",
    "\n",
    "        return frame, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TrainDataset('../data/train_videos/', pd.read_csv('../data/train_targets.csv'), disc_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/3000 [00:00<01:27, 34.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 416, 736])\n",
      "torch.Size([3, 416, 736])\n",
      "torch.Size([3, 416, 736])\n",
      "torch.Size([3, 416, 736])\n",
      "torch.Size([3, 416, 736])\n",
      "torch.Size([3, 416, 736])\n",
      "torch.Size([3, 416, 736])\n",
      "torch.Size([3, 416, 736])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/3000 [00:00<01:32, 32.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 416, 736])\n",
      "torch.Size([3, 416, 736])\n",
      "torch.Size([3, 416, 736])\n",
      "torch.Size([3, 416, 736])\n",
      "torch.Size([3, 416, 736])\n",
      "torch.Size([3, 416, 736])\n",
      "torch.Size([3, 416, 736])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 20/3000 [00:00<01:37, 30.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 416, 736])\n",
      "torch.Size([3, 416, 736])\n",
      "torch.Size([3, 416, 736])\n",
      "torch.Size([3, 416, 736])\n",
      "torch.Size([3, 416, 736])\n",
      "torch.Size([3, 416, 736])\n",
      "torch.Size([3, 416, 736])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 27/3000 [00:00<01:37, 30.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 416, 736])\n",
      "torch.Size([3, 416, 736])\n",
      "torch.Size([3, 416, 736])\n",
      "torch.Size([3, 416, 736])\n",
      "torch.Size([3, 416, 736])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mpass\u001b[39;49;00m\n",
      "File \u001b[0;32m~/traffic-violations-detection/.venv/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "Cell \u001b[0;32mIn[2], line 49\u001b[0m, in \u001b[0;36mTrainDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Use Decord to fetch the frame efficiently\u001b[39;00m\n\u001b[1;32m     48\u001b[0m video_reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideos[vid_idx]\n\u001b[0;32m---> 49\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[43mvideo_reader\u001b[49m\u001b[43m[\u001b[49m\u001b[43mframe_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39masnumpy()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Convert BGR to RGB\u001b[39;00m\n\u001b[1;32m     52\u001b[0m frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(frame, (\u001b[38;5;241m736\u001b[39m, \u001b[38;5;241m416\u001b[39m), interpolation\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mINTER_CUBIC)   \n",
      "File \u001b[0;32m~/traffic-violations-detection/.venv/lib/python3.11/site-packages/decord/video_reader.py:105\u001b[0m, in \u001b[0;36mVideoReader.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m out of bound: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_frame))\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseek_accurate(idx)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/traffic-violations-detection/.venv/lib/python3.11/site-packages/decord/video_reader.py:117\u001b[0m, in \u001b[0;36mVideoReader.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Grab the next frame.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03mReturns\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m \n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43m_CAPI_VideoReaderNextFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mshape:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n",
      "File \u001b[0;32m~/traffic-violations-detection/.venv/lib/python3.11/site-packages/decord/_ffi/_ctypes/function.py:173\u001b[0m, in \u001b[0;36mFunctionBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    171\u001b[0m ret_val \u001b[38;5;241m=\u001b[39m DECORDValue()\n\u001b[1;32m    172\u001b[0m ret_tcode \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int()\n\u001b[0;32m--> 173\u001b[0m check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDECORDFuncCall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtcodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret_tcode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    176\u001b[0m _ \u001b[38;5;241m=\u001b[39m temp_args\n\u001b[1;32m    177\u001b[0m _ \u001b[38;5;241m=\u001b[39m args\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm(dataset):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
